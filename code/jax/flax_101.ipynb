{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Libraries",
   "id": "66f22ab99cf86719"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-09T13:39:23.873454Z",
     "start_time": "2025-07-09T13:39:23.017070Z"
    }
   },
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn # Flax's neural network API\n",
    "from jax import random"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dense Network (Fully Connected Layer)",
   "id": "ecacc818998f8d78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T14:59:04.813489Z",
     "start_time": "2025-07-09T14:59:04.797351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from jax.nn.initializers import constant as jnp_constant\n",
    "\n",
    "class SimpleDenseNetwork(nn.Module):\n",
    "    # Defining the number of features (neurons) for our layer\n",
    "    features: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # nn.Dense is Flax's fully connected layer\n",
    "        # jnp_constant(1.0) will make all biases start at 1.0\n",
    "        x = nn.Dense(\n",
    "            features=self.features, bias_init=jnp_constant(1.0)\n",
    "        )(x)\n",
    "        return x"
   ],
   "id": "34572823339d6f3a",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T14:58:45.348274Z",
     "start_time": "2025-07-09T14:58:45.319400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create an instance of our model\n",
    "model = SimpleDenseNetwork(features=10)\n",
    "\n",
    "# Generate a random key for initialization\n",
    "# Using a fixed seed for reproducibility\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "# Create a dummy input array\n",
    "# 1 sample, 5 input features\n",
    "dummy_input = jnp.ones((1, 5))\n",
    "\n",
    "# Initialize the model's parameters\n",
    "params = model.init(key, dummy_input)['params']\n",
    "\n",
    "print(\"Model parameters:\")\n",
    "print(jax.tree.map(lambda x: x.shape, params))"
   ],
   "id": "16cbceb0ae3c598f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:\n",
      "{'Dense_0': {'bias': (10,), 'kernel': (5, 10)}}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T14:59:17.290110Z",
     "start_time": "2025-07-09T14:59:17.270611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create some actual input data\n",
    "# Batch of 2 samples, each with 5 features\n",
    "input_data = jnp.array([\n",
    "    [1.0, 2.0, 3.0, 4.0, 5.0],\n",
    "    [6.0, 7.0, 8.0, 9.0, 10.0],\n",
    "])\n",
    "print(f\"Input data shape: {input_data.shape}\")\n",
    "\n",
    "# Perform the forward pass\n",
    "output_data = model.apply({'params': params}, input_data)\n",
    "\n",
    "print(\"\\nOutput data:\")\n",
    "print(output_data)\n",
    "print(f\"\\nOutput data shape: {output_data.shape}\")"
   ],
   "id": "d66698d58be04159",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (2, 5)\n",
      "\n",
      "Output data:\n",
      "[[  2.534062    -1.966347    -2.2287571    4.9156427    3.616924\n",
      "    0.08119702   1.7199428   -5.069695     5.1260605    0.6473913 ]\n",
      " [ -0.8696096   -3.6501474  -10.47789      8.267586     8.88211\n",
      "    0.43612492   2.7938604  -14.02945     10.69361      2.9485097 ]]\n",
      "\n",
      "Output data shape: (2, 10)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T14:57:52.336652Z",
     "start_time": "2025-07-09T14:57:52.327071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"--- Weights (kernel) ---\")\n",
    "print(params['Dense_0']['kernel'])\n",
    "print(f\"Shape: {params['Dense_0']['kernel'].shape}\")\n",
    "\n",
    "print(\"\\n--- Biases ---\")\n",
    "print(params['Dense_0']['bias'])\n",
    "print(f\"Shape: {params['Dense_0']['bias'].shape}\")"
   ],
   "id": "71bddff3d8c0a828",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Weights (kernel) ---\n",
      "[[-0.7387071   0.33744565 -0.61614853 -0.38103622  0.4111639   0.5740531\n",
      "   0.4669155  -0.02079375  0.18908004  0.34764266]\n",
      " [-0.81367314  0.38204673 -0.31585133  0.22778194  0.54632616 -0.12296208\n",
      "  -0.6633667  -0.59038585 -0.31810617 -0.01491392]\n",
      " [-0.15151945 -0.2792575  -0.94288903 -0.24101867 -0.5139249  -0.27185342\n",
      "   0.24369092 -0.32793495  0.5703741   0.5477993 ]\n",
      " [ 0.76115257 -0.6548614   0.27755     0.75913733  0.39247712 -0.1098888\n",
      "  -0.01096913 -0.37985826  0.49873945  0.21229894]\n",
      " [ 0.2620127  -0.12213357 -0.05248778  0.30552435  0.2169948   0.00163671\n",
      "   0.1785129  -0.47297826  0.17342253 -0.6326034 ]]\n",
      "Shape: (5, 10)\n",
      "\n",
      "--- Biases ---\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Shape: (10,)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Activation Function",
   "id": "6c240eba4c49a3fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T10:14:35.577897Z",
     "start_time": "2025-07-10T10:14:35.421524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a Multi-Layer Perceptron (MLP)\n",
    "class MLP(nn.Module):\n",
    "    # Number of neurons in the hidden layer\n",
    "    features_hidden: int\n",
    "\n",
    "    # Number of neurons in the output layer\n",
    "    features_output: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # 1. Hidden Layer\n",
    "        # Apply the first Dense Layer\n",
    "        x = nn.Dense(features=self.features_hidden)(x)\n",
    "        # Apply a ReLU activation function\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        # 2. Output Layer\n",
    "        # Apply the second Dense Layer (output layer)\n",
    "        x = nn.Dense(features=self.features_output)(x)\n",
    "\n",
    "        return x"
   ],
   "id": "70556516cd7fe662",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T10:19:29.171940Z",
     "start_time": "2025-07-10T10:19:28.087425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create an instance of our MLP model\n",
    "# 32 neurons in the hidden layer\n",
    "# 10 layers in the output (e.g., for 10 classes)\n",
    "mlp_model = MLP(features_hidden=32, features_output=10)\n",
    "\n",
    "# Generate a random key for initialization (seed - reproducibility)\n",
    "key = random.PRNGKey(132)\n",
    "\n",
    "# Dummy input (batch of 1 sample, 5 input features)\n",
    "dummy_input = jnp.ones((1, 5))\n",
    "\n",
    "# Initialize the MLP's parameters\n",
    "mlp_params = mlp_model.init(key, dummy_input)['params']\n",
    "\n",
    "print(\"MLP Model parameters:\")\n",
    "print(jax.tree.map(lambda x: x.shape, mlp_params))\n",
    "\n",
    "# --- Perform a forward pass with the new data ---\n",
    "input_data_mlp = jnp.array([\n",
    "    [1.0, 2.0, 3.0, 4.0, 5.0],\n",
    "    [-1.0, -7.0, -8.0, 9.0, 10.0],\n",
    "])\n",
    "\n",
    "print(f\"\\n Input data shape: {input_data_mlp.shape}\")\n",
    "\n",
    "# Apply the model with its parameters and the input data\n",
    "output_data_mlp = mlp_model.apply({'params': mlp_params}, input_data_mlp)\n",
    "\n",
    "print(\"\\nOutput data from MLP:\")\n",
    "print(output_data_mlp)\n",
    "print(f\"\\nOutput data shape: {output_data_mlp.shape}\")"
   ],
   "id": "dbc841f13d7b5751",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Model parameters:\n",
      "{'Dense_0': {'bias': (32,), 'kernel': (5, 32)}, 'Dense_1': {'bias': (10,), 'kernel': (32, 10)}}\n",
      "\n",
      " Input data shape: (2, 5)\n",
      "\n",
      "Output data from MLP:\n",
      "[[ 0.10567813 -0.68505734  2.253239   -2.7651613  -1.3348109  -0.9340192\n",
      "   0.35664153 -1.9763117  -1.1373323   2.0080562 ]\n",
      " [ 3.539614   -6.495874   -0.8235873   2.8437166   0.9035323  -6.36709\n",
      "   1.1798904  -5.179701    1.3909954   7.832258  ]]\n",
      "\n",
      "Output data shape: (2, 10)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loss Function",
   "id": "8235586434a175d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T10:32:58.371962Z",
     "start_time": "2025-07-10T10:32:58.037214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# A simple MSE loss function\n",
    "def mse_loss(predictions, targets):\n",
    "    return jnp.mean((predictions - targets)**2)\n",
    "\n",
    "# Example usage\n",
    "dummy_predictions = jnp.array([1.0, 2.0, 3.0])\n",
    "dummy_targets = jnp.array([1.1, 1.9, 3.2])\n",
    "\n",
    "print(f\"Dummy predictions: {dummy_predictions}\")\n",
    "print(f\"Dummy targets: {dummy_targets}\")\n",
    "print(f\"MSE Loss: {mse_loss(dummy_predictions, dummy_targets)}\")"
   ],
   "id": "64d7e640139c95fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy predictions: [1. 2. 3.]\n",
      "Dummy targets: [1.1 1.9 3.2]\n",
      "MSE Loss: 0.02000000886619091\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Optimizer",
   "id": "35ed674d7ffba8c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T10:50:36.129751Z",
     "start_time": "2025-07-10T10:50:36.091513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import optax\n",
    "\n",
    "# Define the optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "# This state is tied to our model's initial parameters\n",
    "opt_state = optimizer.init(mlp_params)\n",
    "\n",
    "print(f\"\\nOptimizer state:\\n\")\n",
    "print(jax.tree.map(lambda x:x.shape, opt_state))"
   ],
   "id": "6f240ed14ae4e971",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizer state:\n",
      "\n",
      "(ScaleByAdamState(count=(), mu={'Dense_0': {'bias': (32,), 'kernel': (5, 32)}, 'Dense_1': {'bias': (10,), 'kernel': (32, 10)}}, nu={'Dense_0': {'bias': (32,), 'kernel': (5, 32)}, 'Dense_1': {'bias': (10,), 'kernel': (32, 10)}}), EmptyState())\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "b66a242d6eca9256"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T11:05:49.923306Z",
     "start_time": "2025-07-10T11:05:49.893557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@jax.jit\n",
    "def train_step(params, opt_state, batch_inputs, batch_targets):\n",
    "    # This is the function we want to differentiate\n",
    "    def loss_fn(curr_params):\n",
    "        pred = mlp_model.apply({'params': curr_params}, batch_inputs)\n",
    "        loss = mse_loss(predictions, batch_targets)\n",
    "        return loss\n",
    "\n",
    "    # Use value_and_grad to get both loss and gradients\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "\n",
    "    # Apply gradients to update the optimizer state and parameters\n",
    "    updt, new_opt_state = optimizer.update(grads, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updt)\n",
    "\n",
    "    return new_params, new_opt_state, loss"
   ],
   "id": "17cb67cc5dafab33",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T10:58:37.800826Z",
     "start_time": "2025-07-10T10:58:36.019322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate synthetic data\n",
    "key_data = random.PRNGKey(567) # For data generation randomness\n",
    "\n",
    "num_samples = 1000\n",
    "input_features = 5\n",
    "output_features = 10 # Matches our MLP output\n",
    "\n",
    "# Generate random inputs\n",
    "X = random.normal(key_data, (num_samples, input_features))\n",
    "\n",
    "# This creates a non-linear relationship for the MLP to learn\n",
    "key_noise, key_target_func = random.split(key_data)\n",
    "true_weights = random.normal(key_target_func, (input_features, output_features))\n",
    "true_bias = random.normal(key_target_func, (output_features,))\n",
    "\n",
    "# A slightly more complex non-linear target\n",
    "Y_true = jnp.dot(X**2, true_weights) + true_bias + random.normal(key_noise, (num_samples, output_features)) * 0.1\n",
    "\n",
    "print(f\"Generated X shape: {X.shape}\")\n",
    "print(f\"Generated Y_true shape: {Y_true.shape}\")"
   ],
   "id": "8a6263918c6504cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated X shape: (1000, 5)\n",
      "Generated Y_true shape: (1000, 10)\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T11:02:26.561372Z",
     "start_time": "2025-07-10T11:02:26.326327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 500\n",
    "\n",
    "print(\"\\n Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # In a real scenario, we'd shuffle and create mini-batches here\n",
    "    # For simplicity, we'll use the whole dataset as one 'batch' for now\n",
    "    batch_inputs = X\n",
    "    batch_targets = Y_true\n",
    "\n",
    "    mlp_params, opt_state, loss = train_step(mlp_params, opt_state, batch_inputs, batch_targets)\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "print(f\"Final Loss: {loss:.4f}\")"
   ],
   "id": "c2db625ba58edfbb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting training...\n",
      "Epoch 50/500, Loss: 15.6692\n",
      "Epoch 100/500, Loss: 12.7086\n",
      "Epoch 150/500, Loss: 10.3679\n",
      "Epoch 200/500, Loss: 8.6159\n",
      "Epoch 250/500, Loss: 7.3812\n",
      "Epoch 300/500, Loss: 6.5138\n",
      "Epoch 350/500, Loss: 5.8393\n",
      "Epoch 400/500, Loss: 5.2482\n",
      "Epoch 450/500, Loss: 4.7049\n",
      "Epoch 500/500, Loss: 4.2103\n",
      "\n",
      "Training complete.\n",
      "Final Loss: 4.2103\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "ff2c00198d556655"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T12:09:33.923836Z",
     "start_time": "2025-07-10T12:09:33.140408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate some new, unseen input data for inference\n",
    "key_inference = jax.random.PRNGKey(999)\n",
    "num_inferences_samples = 5\n",
    "\n",
    "X_new = jax.random.normal(key_inference, (num_inferences_samples, input_features))\n",
    "\n",
    "print(f\"New input data for inference: \\n{X_new}\")\n",
    "print(f\"Shape of new input data: {X_new.shape}\")\n",
    "\n",
    "# Perform inference: apply the trained parameters to the new data\n",
    "predictions = mlp_model.apply({'params': mlp_params}, X_new)\n",
    "\n",
    "print(f\"\\nModel predictions on new data:\\n{predictions}\")\n",
    "print(f\"Shape of new predictions: {predictions.shape}\")"
   ],
   "id": "f2899e5321907cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New input data for inference: \n",
      "[[-1.898162    1.3126683  -0.93570435 -0.40194905  1.1873449 ]\n",
      " [ 0.8682871   0.05332198 -1.2655848  -0.32980207  0.24419203]\n",
      " [-1.7266408  -0.6284628   2.944953   -1.3948829   0.62099993]\n",
      " [ 0.90469366  0.6898744  -2.0962944  -0.9236466  -0.19609946]\n",
      " [ 0.38934988  0.43965247 -1.1499423  -0.9559521   1.2161872 ]]\n",
      "Shape of new input data: (5, 5)\n",
      "\n",
      "Model predictions on new data:\n",
      "[[ -7.53619     -8.692844    -1.9360822   -2.9083357   -1.6106429\n",
      "    2.0235376    1.2659616    1.0644199   -3.0131845   -1.6381968 ]\n",
      " [ -5.230658    -5.1156707   -2.6157258   -2.3324254   -1.6410153\n",
      "    1.3128057    2.5697365    0.5646908   -0.8015749   -1.9574323 ]\n",
      " [-10.263551   -10.581922    -4.466531    -5.645301    -3.8386297\n",
      "    3.1153164    5.514456     0.7701984   -2.111065    -4.586107  ]\n",
      " [ -8.351736    -7.9151163   -4.1317945   -4.149193    -2.7879117\n",
      "    2.2939289    3.5006623   -0.6195143    0.1408867   -3.268793  ]\n",
      " [ -6.4102254   -6.752006    -2.709612    -0.9659612    0.35745344\n",
      "    1.6571534    0.934777     0.8962714   -2.1535032    0.22672668]]\n",
      "Shape of new predictions: (5, 10)\n"
     ]
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
